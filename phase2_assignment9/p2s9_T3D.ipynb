{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "p2s9_T3D.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mmazssh9ZzYI",
        "colab_type": "text"
      },
      "source": [
        "### T3D Implementation Steps\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "29O1LSaYaDGk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "### Initialization\n",
        "# Import essential packages\n",
        "import os\n",
        "import time\n",
        "import random\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import gym\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from gym import wrappers\n",
        "from torch.autograd import Variable\n",
        "from collections import deque\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4e3Xms_Afsp8",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 101
        },
        "outputId": "6aeb9f5d-5a05-4bf2-d636-6b1c813b5bc3"
      },
      "source": [
        "package = \"pybullet_envs\"\n",
        "try:\n",
        "    __import__(package)\n",
        "except ImportError:\n",
        "    !pip install pybullet"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting pybullet\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d8/ac/a422ab8d1c57ab3f43e573b5a5f532e6afd348d81308fe66a1ecb691548e/pybullet-2.7.1-cp36-cp36m-manylinux1_x86_64.whl (95.0MB)\n",
            "\u001b[K     |████████████████████████████████| 95.0MB 62kB/s \n",
            "\u001b[?25hInstalling collected packages: pybullet\n",
            "Successfully installed pybullet-2.7.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "laSZf-_ve0QA",
        "colab_type": "text"
      },
      "source": [
        "# STEP 1 \n",
        "### We initialize the Experience Replay Memory with a size of 1e6.\n",
        "### Then we populate it with new transitions\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bEA1p05Ae4lE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class ReplayBuffer(object):\n",
        "\tdef __init__(self, max_size = 1e6):\n",
        "\t\tself.storage = []\n",
        "\t\tself.max_size = max_size\n",
        "\t\tself.ptr = 0\n",
        "\t\t\n",
        "\tdef add(self, transition):\n",
        "\t\tif len(self.storage) == self.max_size:\n",
        "\t\t\tself.storage[int(self.ptr)] = trasition\n",
        "\t\t\tself.ptr = (self.ptr + 1) % self.max_size\n",
        "\t\telse:\n",
        "\t\t\tself.storage.append(transition)\n",
        "\t\n",
        "\tdef sample(self, batch_size):\n",
        "\t\tind = np.random.randint(0, len(self.storage), batch_size)\n",
        "\t\tbatch_states, batch_next_states, batch_actions, batch_rewards, batch_done = [], [], [], []\n",
        "\t\tfor i in ind:\n",
        "\t\t\tstate, next_state, action, reward, done = self.storage[i]\n",
        "\t\t\tbatch_states.append(np.array(state, copy=False))\n",
        "\t\t\tbatch_next_states.append(np.array(next_state, copy=False))\n",
        "\t\t\tbatch_actions.append(np.array(action, copy=False))\n",
        "\t\t\tbatch_rewards.append(np.array(reward, copy = False))\n",
        "\t\t\tbatch_done.append(np.array(done, copy = False))\n",
        "\t\treturn np.array(batch_states), np.array(batch_next_states), np.array(batch_actions), np.array(batch_rewards).reshape(-1,1),np.array(batch_done).reshape(-1,1)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XbOzm1Z4e_wQ",
        "colab_type": "text"
      },
      "source": [
        "# STEP 2 \n",
        "### Build one DNN for the Actor model and one for Actor Target\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BCH0ai0mfHf2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Actor(nn.Module):\n",
        "\tdef __init__(self, state_dims, action_dim, max_action):\n",
        "\t\t# max_action is to clip in case we added too much noise\n",
        "\t\tsuper(Actor, self).__init__() # activate the inheritance\n",
        "\t\tself.layer_1 = nn.Linear(state_dims, 400)\n",
        "\t\tself.layer_2 = nn.Linear(400, 300)\n",
        "\t\tself.Layer_3 = nn.Linear(300, action_dim)\n",
        "\t\tself.max_action = max_action\n",
        "\t\t\n",
        "\tdef forward(self,x):\n",
        "\t\tx = F.relu(self.layer_1(x))\n",
        "\t\tx = F.relu(self.layer_2(x))\n",
        "\t\tx = self.max_action * torch.tanh(self.layer_3(x))\n",
        "\t\treturn x\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8kq-KKwQhKrn",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "## \tSTEP 3 \n",
        "### Build two DNNs for the two Critic models and two DNNs for the two Critic Targets\t\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FxGk0ntomD5t",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "class Critic(nn.Module):\n",
        "\n",
        "\tdef __init__(self, state_dims, action_dim):\n",
        "\t\t#max_action is to clip in case we added too much noise\n",
        "\t\tsuper(Critic, self).__init__()\n",
        "\t\t# First Critic Network\n",
        "\t\tself.layer_1 = nn.Linear(state_dims + action_dim, 400)\n",
        "\t\tself.layer_2 = nn.Linear(400, 300)\n",
        "\t\tself.Layer_3 = nn.Linear(300, action_dim)\n",
        "\t\t#Second Critic Network\n",
        "\t\tself.layer_4 = nn.Linear(state_dims + action_dim, 400)\n",
        "\t\tself.layer_5 = nn.Linear(400, 300)\n",
        "\t\tself.Layer_6 = nn.Linear(300, action_dim)\n",
        "\t\t\n",
        "\tdef forward(self,x): # x - state, u-action\n",
        "\t\txu = torch.cat([x,u],1) #1 for vertical concatenation, 0 for horizondal\n",
        "\t\t# forward propagation on first Critic\n",
        "\t\tx1 = F.relu(self.layer_1(xu))\n",
        "\t\tx1 = F.relu(self.layer_2(x1))\n",
        "\t\tx1 = self.layer_3(x1)\n",
        "\t\t# forward propagation on second Critic\n",
        "\t\tx2 = F.relu(self.layer_4(xu))\n",
        "\t\tx2 = F.relu(self.layer_5(x2))\n",
        "\t\tx2 = self.layer_6(x2)\n",
        "\t\treturn x1,x2\n",
        "\t\t\n",
        "\tdef Q1(self, x,u ): # x-state, u=action This is used for updating the Q values\n",
        "\t\txu = torch.cat([x,u],1)\n",
        "\t\tx1 = F.relu(self.layer_1(xu))\n",
        "\t\tx1 = F.relu(self.layer_2(x1))\n",
        "\t\tx1 = self.layer_3(x1)\n",
        "\t\treturn x1\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "99USgzX0mJQw",
        "colab_type": "text"
      },
      "source": [
        "### STEP 4-15 \n",
        "### Training process. Create a T3D class, initialize variables and get ready for step 4\t\t\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8s-KXlwom0yc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#device = torch.device('cuda', if torch.cuda.is_available() else 'cpu')\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XuU8PMPMZ6ts",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "# Building the whole training process into a class\n",
        "class T3D(object):\n",
        "\tdef __init__(self, state_dims, action_dim, max_action):\n",
        "\t\t# making sure our T3D class can work with any environment\n",
        "\t\tself.actor = Actor(state_dims, action_dim, max_action).to(device) # GD\n",
        "\t\tself.action_target = Actor(state_dims, action_dim, max_action).to(device) # Polyak Avg\n",
        "\t\tself.actor_target.load_state_dict(self.actor.state_dict)\n",
        "\t\t#initializing with model weights to keep them same\n",
        "\t\tself.actor_optimizer = torch.option.Adam(self.actor.parameters())\n",
        "\t\t\n",
        "\t\tself.critic = Critic(state_dims, action_dim).to(device) # GD\n",
        "\t\tself.critic_target = critic(state_dims, action_dim).to(device) # ployak Avg\n",
        "\t\tself.critic_target.load_state_dict(self.critic.state_dict)\n",
        "\t\t# initializing with model weights to keep them same\n",
        "\t\tself.critic_optimizer = torch.option.Adam(self.critic.parameters())\n",
        "\t\tself.max_action = max_action\n",
        "\t\t\n",
        "\tdef select_action(self, state):\n",
        "\t\tstate = torch.Tensor(state.reshape(1,-1)).to(device)\n",
        "\t\treturn self.actor(state).cpu().data.numpy().flatten()\n",
        "\t\t# need to convert to numpy, remember clipping?\n",
        "\t\t\n",
        "\t\t\n",
        "\t### STEP 4 \n",
        "\t### Sample from a batch of transitions (s, s', a, r) from the memory\n",
        "\tdef train(self, replay_buffer, iterations, batch_size=100, discount = 0.99, tau=0.005, policy_noise=0.2, noise_clip=0.5, policy_freq=2):\n",
        "\t\tfor it in range(iterations):\n",
        "\t\t\t# setp 4 we sample from a batch of transition(s, s', a, r) from memory\n",
        "\t\t\tbatch_states, batch_next_states, batch_actions, batch_rewards, batch_dones = replay_buffer.sample(batch_size)\n",
        "\t\t\tstate = torch.Tensor(batch_states).to(device)\n",
        "\t\t\tnext_state = torch.Tensor(batch_next_states).to(device)\n",
        "\t\t\taction = torch.Tensor(batch_actions).to(device)\n",
        "\t\t\treward = torch.Tensor(batch_rewards).to(device)\n",
        "\t\t\tdone = torch.Tensor(batch_dones).to(device)\n",
        "\t\t\t\n",
        "\t\t\t#  STEP 5 \n",
        "\t\t\t# From the next state s', the actor target plays the next action a'\n",
        "\t\t\tnext_state = self.actor_target.forward(next_state)\n",
        "\t\t\t\n",
        "\t\t\t# STEP 6 \n",
        "\t\t\t#We add Gaussian noise to this next action a' and we clamp it in a range of values supported by the environment\n",
        "\t\t\t\n",
        "\t\t\tnoise = torch.Tensor(batch_actions).data.normal_(0, policy_noise).to(device)\n",
        "\t\t\tnoise = noise.clamp(-noise_clip, noise_clip)\n",
        "\t\t\tnext_action = (next_action+noise).clamp(-self.max_Action, self.max_action)\n",
        "\t\t\t\n",
        "\t\t\t# STEP 7 \n",
        "\t\t\t#The two Critic targets take each the couple (s', a') as input and return two Q values, Qt1(s', a') and Qt2(s', a') as outputs\n",
        "\t\t\t\n",
        "\t\t\ttarget_Q1, target_Q2 = self.critic_target.formward(next_state, next_action)\n",
        "\t\t\t\n",
        "\t\t\t#  STEP 8 \n",
        "\t\t\t# Keep the minimum of these two Q-Values\n",
        "\t\t\ttarget_Q = torch.min(target_Q1, target_Q2)\n",
        "\t\t\t\n",
        "\t\t\t# STEP 9 \n",
        "\t\t\t#We get the final target of the two Critic models, #which is:\n",
        "\t\t\t# Qt = r + gamma * min(Qt1, Qt2)\n",
        "\t\t\t# We can define \n",
        "\t\t\t#target_q or Qt as reward + discount  * torch.min(Qt1, Qt2)\n",
        "\t\t\t# target_Q = reward +(1-done)*discount*target_Q\n",
        "\t\t\t# 0 = episode not over, 1 = episode over\n",
        "\t\t\t# we can't run the above equation efficiently as some components are in computational graphs and some are not. we need to mane one minor modification\n",
        "\t\t\ttarget_Q = reward + ((1-done)*discount * target_Q).detach()\n",
        "\n",
        "\t\t\t### STEP 10 \n",
        "\t\t\t### Two critic models take (s, a) and return two Q-Vales\n",
        "\t\t\tcurrnet_Q1, currnet_Q2 = self.critic.forward(state, action)\n",
        "\t\t\t# STEP 11 \n",
        "\t\t\t# Compute the Critic Loss\n",
        "\t\t\tcritic_loss = F.mse_loss(currnet_Q1, target_Q) + F.mse_loss(currnet_Q2, target_Q)\n",
        "\t\t\t\n",
        "\t\t\t# STEP 12 \n",
        "\t\t\t### Backpropagate this critic loss and update the parameters of two Critic models\n",
        "\t\t\tself.critic_optimizer.zero_grad() # initializing the gradients to zero\n",
        "\t\t\tcritic_loss.backward() # computing the graidents\n",
        "\t\t\tself.critic_optimizer.step() # performing the weight updates\n",
        "\t\t\t\n",
        "\t\t\t# STEP 13 \n",
        "\t\t\t#Once every two iterations, we update our Actor model by performing gradient ASCENT on the output of the first Critic model\n",
        "\t\t\t\n",
        "\t\t\tif it % policy_freq == 0:\n",
        "\t\t\t\t# This is DPG partition\n",
        "\t\t\t\tactor_loss = -(self.critic.Q1(state, self.actor(state)).mean())\n",
        "\t\t\t\tself.actor_optimizer.grad_zero()\n",
        "\t\t\t\tactor_loss.backward()\n",
        "\t\t\t\tself.actor_optimizer.step()\n",
        "\t\t\t\t\n",
        "\t\t\t# STEP 14 \n",
        "\t\t\t#Still, in once every two iterations, we update our Actor Target by Polyak Averaging\t\n",
        "\t\t\tfor param, target_param in zip(self.actor.parameters(), self.actor_target.parameters()):\n",
        "\t\t\t\ttarget_param.data.copy(tau * param.data + (1-tau) * target_param.data)\n",
        "\t\t\t#  STEP 15 \n",
        "\t\t\t# Still, in once every two iterations, we update our  Critic Target by Polyak Averaging\t\n",
        "\t\t\tfor param, target_param in zip(self.critic.parameters(), self.critic_target.parameters()):\n",
        "\t\t\t\ttarget_param.data.copy(tau * param.data + (1-tau) * target_param.data)\n",
        "\t\t\t\n",
        "\t\t\t# T3D is done now!!!\n"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}